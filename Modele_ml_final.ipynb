{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "dataset_path = \"/Users/nathanserfaty/.cache/kagglehub/datasets/borismarjanovic/price-volume-data-for-all-us-stocks-etfs/versions/3\"\n",
    "subdirs = [\"Stocks\", \"ETFs\"]\n",
    "\n",
    "\n",
    "available_subdirs = {subdir: os.path.join(dataset_path, subdir) for subdir in subdirs if os.path.exists(os.path.join(dataset_path, subdir))}\n",
    "\n",
    "\n",
    "stock_files = os.listdir(available_subdirs[\"Stocks\"]) if \"Stocks\" in available_subdirs else []\n",
    "etf_files = os.listdir(available_subdirs[\"ETFs\"]) if \"ETFs\" in available_subdirs else []\n",
    "\n",
    "def load_data_with_dates(files, subdir):\n",
    "    subdir_path = available_subdirs.get(subdir, None)\n",
    "    asset_data = []\n",
    "\n",
    "    if subdir_path:\n",
    "        for file in files:\n",
    "            file_path = os.path.join(subdir_path, file)\n",
    "\n",
    "            if file.endswith(\".txt\"):\n",
    "                try:\n",
    "                    if os.path.getsize(file_path) > 0:\n",
    "                        df = pd.read_csv(file_path)\n",
    "\n",
    "                        if 'Date' in df.columns:\n",
    "                            df['Date'] = pd.to_datetime(df['Date'])\n",
    "                            df = df.sort_values(by='Date')\n",
    "\n",
    "                            # Ajouter l'asset comme colonne\n",
    "                            df['Asset'] = file.replace('.txt', '')\n",
    "\n",
    "                            # Ajouter aux donn√©es filtr√©es\n",
    "                            asset_data.append({\n",
    "                                'df': df,\n",
    "                                'start_date': df['Date'].min(),\n",
    "                                'end_date': df['Date'].max()\n",
    "                            })\n",
    "                except pd.errors.EmptyDataError:\n",
    "                    print(f\"‚ö†Ô∏è Fichier vide ignor√© : {file_path}\")\n",
    "\n",
    "    return asset_data\n",
    "\n",
    "\n",
    "stocks_data = load_data_with_dates(stock_files, \"Stocks\")\n",
    "etfs_data = load_data_with_dates(etf_files, \"ETFs\")\n",
    "\n",
    "\n",
    "max_date_stocks = max([data['end_date'] for data in stocks_data])\n",
    "max_date_etfs = max([data['end_date'] for data in etfs_data])\n",
    "\n",
    "\n",
    "filtered_stocks = [\n",
    "    data['df'] for data in stocks_data\n",
    "    if pd.Timestamp(\"1960-01-01\") <= data['start_date'] <= pd.Timestamp(\"1990-12-31\")\n",
    "    and data['end_date'] == max_date_stocks\n",
    "]\n",
    "\n",
    "filtered_etfs = [\n",
    "    data['df'] for data in etfs_data\n",
    "    if pd.Timestamp(\"1995-01-01\") <= data['start_date'] <= pd.Timestamp(\"2005-12-31\")\n",
    "    and data['end_date'] == max_date_etfs\n",
    "]\n",
    "\n",
    "print(f\"üìä Nombre d'actions disponibles apr√®s filtrage : {len(filtered_stocks)}\")\n",
    "print(f\"üìä Nombre d'ETFs disponibles apr√®s filtrage : {len(filtered_etfs)}\")\n",
    "\n",
    "\n",
    "filtered_stocks = random.sample(filtered_stocks, min(60, len(filtered_stocks)))\n",
    "filtered_etfs = random.sample(filtered_etfs, min(160, len(filtered_etfs)))\n",
    "\n",
    "\n",
    "df_stocks = pd.concat(filtered_stocks, ignore_index=True) if filtered_stocks else pd.DataFrame()\n",
    "df_etfs = pd.concat(filtered_etfs, ignore_index=True) if filtered_etfs else pd.DataFrame()\n",
    "\n",
    "\n",
    "print(\"‚úÖ Nombre d'actions retenues :\", df_stocks['Asset'].nunique() if not df_stocks.empty else 0)\n",
    "print(\"‚úÖ Nombre d'ETFs retenus :\", df_etfs['Asset'].nunique() if not df_etfs.empty else 0)\n",
    "\n",
    "df_stocks.head(), df_etfs.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.scatterplot(x=df_stocks['Date'], y=df_stocks['Asset'], alpha=0.5, color=\"blue\")\n",
    "plt.title(\"R√©partition des Actions par Date\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Actions\")\n",
    "\n",
    "# R√©duire le nombre de labels pour √©viter le chevauchement\n",
    "plt.yticks(rotation=0, fontsize=8)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.scatterplot(x=df_etfs['Date'], y=df_etfs['Asset'], alpha=0.5, color=\"red\")\n",
    "plt.title(\"R√©partition des ETFs par Date\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"ETFs\")\n",
    "\n",
    "# R√©duire le nombre de labels pour √©viter le chevauchement\n",
    "plt.yticks(rotation=0, fontsize=8)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_stocks.columns)\n",
    "print(df_etfs.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse de donn√©es\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Supprimer les valeurs manquantes\n",
    "df_stocks_clean = df_stocks.dropna()\n",
    "df_etfs_clean = df_etfs.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Colonnes disponibles dans df_stocks:\", df_stocks_clean.columns)\n",
    "print(\"Colonnes disponibles dans df_etfs:\", df_etfs_clean.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stocks_perf = df_stocks_clean.groupby(\"Asset\")['Close'].agg(['first', 'last'])\n",
    "df_stocks_perf['Return'] = (df_stocks_perf['last'] - df_stocks_perf['first']) / df_stocks_perf['first'] * 100\n",
    "df_stocks_perf = df_stocks_perf.sort_values(by=\"Return\", ascending=False).head(10)\n",
    "\n",
    "df_etfs_perf = df_etfs_clean.groupby(\"Asset\")['Close'].agg(['first', 'last'])\n",
    "df_etfs_perf['Return'] = (df_etfs_perf['last'] - df_etfs_perf['first']) / df_etfs_perf['first'] * 100\n",
    "df_etfs_perf = df_etfs_perf.sort_values(by=\"Return\", ascending=False).head(10)\n",
    "\n",
    "# Affichage des performances des actions et ETFs\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "sns.barplot(x=df_stocks_perf.index, y=df_stocks_perf['Return'], ax=axes[0], palette=\"Blues_r\")\n",
    "axes[0].set_title(\"Top 10 des actions les plus performantes (%)\")\n",
    "axes[0].set_xticklabels(df_stocks_perf.index, rotation=45)\n",
    "\n",
    "sns.barplot(x=df_etfs_perf.index, y=df_etfs_perf['Return'], ax=axes[1], palette=\"Greens_r\")\n",
    "axes[1].set_title(\"Top 10 des ETFs les plus performants (%)\")\n",
    "axes[1].set_xticklabels(df_etfs_perf.index, rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volatility_stocks = df_stocks_clean.groupby(\"Asset\")['Close'].std().mean()\n",
    "volatility_etfs = df_etfs_clean.groupby(\"Asset\")['Close'].std().mean()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=[\"Actions\", \"ETFs\"], y=[volatility_stocks, volatility_etfs], palette=[\"blue\", \"green\"])\n",
    "plt.title(\"Volatilit√© moyenne des actions et des ETFs\")\n",
    "plt.ylabel(\"√âcart-type du prix de cl√¥ture\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recharger les biblioth√®ques n√©cessaires apr√®s la r√©initialisation\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Red√©finir les DataFrames (df_stocks_clean et df_etfs_clean) manquants apr√®s la r√©initialisation\n",
    "# L'utilisateur doit fournir √† nouveau les fichiers si n√©cessaire\n",
    "\n",
    "# Calcul de la m√©diane des prix de cl√¥ture pour les actions et les ETFs\n",
    "median_stock_price = df_stocks_clean['Close'].median()\n",
    "median_etf_price = df_etfs_clean['Close'].median()\n",
    "\n",
    "# Affichage sous forme de graphique\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=[\"Actions\", \"ETFs\"], y=[median_stock_price, median_etf_price], palette=[\"blue\", \"green\"])\n",
    "plt.title(\"Comparaison de la m√©diane des prix de cl√¥ture des actions et des ETFs\")\n",
    "plt.ylabel(\"M√©diane du prix de cl√¥ture\")\n",
    "plt.show()\n",
    "\n",
    "# Affichage des valeurs num√©riques pour plus de clart√©\n",
    "median_stock_price, median_etf_price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution de la m√©diane des prix de cl√¥ture par entreprise\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(df_stocks_clean.groupby(\"Asset\")['Close'].median(), bins=50, kde=True, color=\"blue\", label=\"Actions\")\n",
    "sns.histplot(df_etfs_clean.groupby(\"Asset\")['Close'].median(), bins=50, kde=True, color=\"green\", label=\"ETFs\")\n",
    "plt.title(\"Distribution des m√©dianes des prix de cl√¥ture par entreprise (Actions vs ETFs)\")\n",
    "plt.xlabel(\"M√©diane du prix de cl√¥ture\")\n",
    "plt.ylabel(\"Fr√©quence\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Boxplot de la m√©diane des prix de cl√¥ture par entreprise\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=[df_stocks_clean.groupby(\"Asset\")['Close'].median(), df_etfs_clean.groupby(\"Asset\")['Close'].median()], palette=[\"blue\", \"green\"])\n",
    "plt.xticks([0, 1], [\"Actions\", \"ETFs\"])\n",
    "plt.title(\"Boxplot des m√©dianes des prix de cl√¥ture (Actions vs ETFs)\")\n",
    "plt.ylabel(\"M√©diane du prix de cl√¥ture\")\n",
    "plt.show()\n",
    "\n",
    "# Distribution de la volatilit√© (√©cart-type) des prix de cl√¥ture par entreprise\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(df_stocks_clean.groupby(\"Asset\")['Close'].std(), bins=50, kde=True, color=\"blue\", label=\"Actions\")\n",
    "sns.histplot(df_etfs_clean.groupby(\"Asset\")['Close'].std(), bins=50, kde=True, color=\"green\", label=\"ETFs\")\n",
    "plt.title(\"Distribution de la volatilit√© des prix de cl√¥ture par entreprise (Actions vs ETFs)\")\n",
    "plt.xlabel(\"√âcart-type du prix de cl√¥ture\")\n",
    "plt.ylabel(\"Fr√©quence\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Boxplot de la volatilit√© des prix de cl√¥ture par entreprise\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=[df_stocks_clean.groupby(\"Asset\")['Close'].std(), df_etfs_clean.groupby(\"Asset\")['Close'].std()], palette=[\"blue\", \"green\"])\n",
    "plt.xticks([0, 1], [\"Actions\", \"ETFs\"])\n",
    "plt.title(\"Boxplot de la volatilit√© des prix de cl√¥ture (Actions vs ETFs)\")\n",
    "plt.ylabel(\"√âcart-type du prix de cl√¥ture\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "\n",
    "# S√©lectionner les 10 meilleures actions et ETFs\n",
    "top_10_stocks = df_stocks_clean[df_stocks_clean['Asset'].isin(df_stocks_perf.index)]\n",
    "top_10_etfs = df_etfs_clean[df_etfs_clean['Asset'].isin(df_etfs_perf.index)]\n",
    "\n",
    "# Cr√©er des graphiques de distribution pour chaque actif du top 10\n",
    "fig, axes = plt.subplots(5, 2, figsize=(14, 20))\n",
    "\n",
    "for i, asset in enumerate(df_stocks_perf.index):\n",
    "    data = top_10_stocks[top_10_stocks['Asset'] == asset]['Close']\n",
    "    sns.histplot(data, bins=30, kde=True, ax=axes[i//2, i%2], color=\"blue\")\n",
    "    axes[i//2, i%2].set_title(f\"Distribution du prix de cl√¥ture - {asset} (Action)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig, axes = plt.subplots(5, 2, figsize=(14, 20))\n",
    "\n",
    "for i, asset in enumerate(df_etfs_perf.index):\n",
    "    data = top_10_etfs[top_10_etfs['Asset'] == asset]['Close']\n",
    "    sns.histplot(data, bins=30, kde=True, ax=axes[i//2, i%2], color=\"green\")\n",
    "    axes[i//2, i%2].set_title(f\"Distribution du prix de cl√¥ture - {asset} (ETF)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Tester l'ajustement aux lois normales\n",
    "distributions = ['norm', 'lognorm', 'expon', 'gamma']\n",
    "fitted_results = {}\n",
    "\n",
    "for asset in df_stocks_perf.index:\n",
    "    data = top_10_stocks[top_10_stocks['Asset'] == asset]['Close']\n",
    "    best_fit = {}\n",
    "    \n",
    "    for dist_name in distributions:\n",
    "        dist = getattr(stats, dist_name)\n",
    "        params = dist.fit(data)\n",
    "        ks_stat, p_value = stats.kstest(data, dist_name, args=params)\n",
    "        best_fit[dist_name] = p_value\n",
    "\n",
    "    fitted_results[asset] = max(best_fit, key=best_fit.get)\n",
    "\n",
    "for asset in df_etfs_perf.index:\n",
    "    data = top_10_etfs[top_10_etfs['Asset'] == asset]['Close']\n",
    "    best_fit = {}\n",
    "    \n",
    "    for dist_name in distributions:\n",
    "        dist = getattr(stats, dist_name)\n",
    "        params = dist.fit(data)\n",
    "        ks_stat, p_value = stats.kstest(data, dist_name, args=params)\n",
    "        best_fit[dist_name] = p_value\n",
    "\n",
    "    fitted_results[asset] = max(best_fit, key=best_fit.get)\n",
    "\n",
    "# Affichage des r√©sultats\n",
    "fitted_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cr√©ation des features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moyennes Mobiles\n",
    "df_stocks_clean['SMA_10'] = df_stocks_clean.groupby('Asset')['Close'].transform(lambda x: x.rolling(window=10).mean())\n",
    "df_stocks_clean['SMA_50'] = df_stocks_clean.groupby('Asset')['Close'].transform(lambda x: x.rolling(window=50).mean())\n",
    "\n",
    "df_etfs_clean['SMA_10'] = df_etfs_clean.groupby('Asset')['Close'].transform(lambda x: x.rolling(window=10).mean())\n",
    "df_etfs_clean['SMA_50'] = df_etfs_clean.groupby('Asset')['Close'].transform(lambda x: x.rolling(window=50).mean())\n",
    "\n",
    "# Volatilit√© (√âcart-type sur 10 jours)\n",
    "df_stocks_clean['Volatility_10'] = df_stocks_clean.groupby('Asset')['Close'].transform(lambda x: x.rolling(window=10).std())\n",
    "df_etfs_clean['Volatility_10'] = df_etfs_clean.groupby('Asset')['Close'].transform(lambda x: x.rolling(window=10).std())\n",
    "\n",
    "# Momentum\n",
    "df_stocks_clean['Momentum_10'] = df_stocks_clean['Close'] - df_stocks_clean.groupby('Asset')['Close'].shift(10)\n",
    "df_etfs_clean['Momentum_10'] = df_etfs_clean['Close'] - df_etfs_clean.groupby('Asset')['Close'].shift(10)\n",
    "\n",
    "# RSI (Relative Strength Index)\n",
    "def compute_rsi(series, window=14):\n",
    "    delta = series.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "df_stocks_clean['RSI_14'] = df_stocks_clean.groupby('Asset')['Close'].transform(lambda x: compute_rsi(x))\n",
    "df_etfs_clean['RSI_14'] = df_etfs_clean.groupby('Asset')['Close'].transform(lambda x: compute_rsi(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D√©finition de la Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stocks_clean['Target'] = (df_stocks_clean.groupby('Asset')['Close'].shift(-1) > df_stocks_clean['Close']).astype(int)\n",
    "df_etfs_clean['Target'] = (df_etfs_clean.groupby('Asset')['Close'].shift(-1) > df_etfs_clean['Close']).astype(int)\n",
    "\n",
    "# Suppression des valeurs NaN apr√®s les transformations\n",
    "df_stocks_clean.dropna(inplace=True)\n",
    "df_etfs_clean.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection + corr Matrix (presentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Open', 'High', 'Low', 'Close', 'SMA_10', 'SMA_50', 'Volatility_10', 'Momentum_10', 'RSI_14']\n",
    "corr_matrix_stocks = df_stocks_clean[features].corr()\n",
    "corr_matrix_etfs = df_etfs_clean[features].corr()\n",
    "selected_features_stocks = features\n",
    "selected_features_etfs = features\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "sns.heatmap(corr_matrix_stocks, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5, ax=axes[0])\n",
    "axes[0].set_title(\"Matrice de Corr√©lation - Actions (Stocks)\")\n",
    "\n",
    "sns.heatmap(corr_matrix_etfs, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5, ax=axes[1])\n",
    "axes[1].set_title(\"Matrice de Corr√©lation - ETFs\")\n",
    "\n",
    "# Ajustement de l'affichage\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_stocks = df_stocks_clean[selected_features_stocks]\n",
    "y_stocks = df_stocks_clean['Target']\n",
    "X_etfs = df_etfs_clean[selected_features_etfs]\n",
    "y_etfs = df_etfs_clean['Target']\n",
    "\n",
    "X_train_stocks, X_test_stocks, y_train_stocks, y_test_stocks = train_test_split(X_stocks, y_stocks, test_size=0.2, random_state=42, shuffle=False)\n",
    "X_train_etfs, X_test_etfs, y_train_etfs, y_test_etfs = train_test_split(X_etfs, y_etfs, test_size=0.2, random_state=42, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R√©partition de la target par asset et etf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Comptage du nombre de 0 et 1 dans Target group√© par Asset\n",
    "target_distribution = df_stocks_clean.groupby(\"Asset\")[\"Target\"].value_counts().unstack(fill_value=0)\n",
    "\n",
    "# Renommage des colonnes pour plus de clart√©\n",
    "target_distribution.columns = [\"Target_0\", \"Target_1\"]\n",
    "\n",
    "# Affichage du DataFrame\n",
    "print(target_distribution)  # Affiche les 10 premi√®res lignes pour √©viter trop d'affichage\n",
    "\n",
    "# Visualisation avec un barplot empil√©\n",
    "plt.figure(figsize=(15, 6))\n",
    "target_distribution.plot(kind=\"bar\", stacked=True, figsize=(15, 6), width=0.8)\n",
    "plt.title(\"R√©partition des cibles (Target) par Asset, action\")\n",
    "plt.xlabel(\"Asset\")\n",
    "plt.ylabel(\"Nombre d'occurrences\")\n",
    "plt.legend([\"Target 0\", \"Target 1\"])\n",
    "plt.xticks(rotation=90)  # Rotation des labels pour plus de lisibilit√©\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Comptage du nombre de 0 et 1 dans Target group√© par Asset\n",
    "target_distribution = df_etfs_clean.groupby(\"Asset\")[\"Target\"].value_counts().unstack(fill_value=0)\n",
    "\n",
    "# Renommage des colonnes pour plus de clart√©\n",
    "target_distribution.columns = [\"Target_0\", \"Target_1\"]\n",
    "\n",
    "# Affichage du DataFrame\n",
    "print(target_distribution)  # Affiche les 10 premi√®res lignes pour √©viter trop d'affichage\n",
    "\n",
    "# Visualisation avec un barplot empil√©\n",
    "plt.figure(figsize=(15, 6))\n",
    "target_distribution.plot(kind=\"bar\", stacked=True, figsize=(15, 6), width=0.8)\n",
    "plt.title(\"R√©partition des cibles (Target) par etf\")\n",
    "plt.xlabel(\"etf\")\n",
    "plt.ylabel(\"Nombre d'occurrences\")\n",
    "plt.legend([\"Target 0\", \"Target 1\"])\n",
    "plt.xticks(rotation=90)  # Rotation des labels pour plus de lisibilit√©\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrainement des mod√®les"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "models_stocks = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")\n",
    "}\n",
    "\n",
    "print(\"\\nüìä R√©sultats des mod√®les - Actions (Stocks) üìä\")\n",
    "for name, model in models_stocks.items():\n",
    "    model.fit(X_train_stocks, y_train_stocks)\n",
    "    y_pred = model.predict(X_test_stocks)\n",
    "    \n",
    "    acc = accuracy_score(y_test_stocks, y_pred)\n",
    "    precision = precision_score(y_test_stocks, y_pred)\n",
    "    recall = recall_score(y_test_stocks, y_pred)\n",
    "    f1 = f1_score(y_test_stocks, y_pred)\n",
    "    \n",
    "    print(f\"{name} - Accuracy: {acc:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
    "\n",
    "models_etfs = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")\n",
    "}\n",
    "\n",
    "print(\"\\nüìä R√©sultats des mod√®les - ETFs üìä\")\n",
    "for name, model in models_etfs.items():\n",
    "    model.fit(X_train_etfs, y_train_etfs)\n",
    "    y_pred = model.predict(X_test_etfs)\n",
    "    \n",
    "    acc = accuracy_score(y_test_etfs, y_pred)\n",
    "    precision = precision_score(y_test_etfs, y_pred)\n",
    "    recall = recall_score(y_test_etfs, y_pred)\n",
    "    f1 = f1_score(y_test_etfs, y_pred)\n",
    "    \n",
    "    print(f\"{name} - Accuracy: {acc:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimisation avec grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "param_grids = {\n",
    "    \"Logistic Regression\": {\n",
    "        \"C\": [0.01, 0.1, 1, 10, 100],  # R√©gularisation\n",
    "        \"solver\": [\"lbfgs\", \"liblinear\"]  # Algorithme de r√©solution\n",
    "    },\n",
    "    \"KNN\": {\n",
    "        \"n_neighbors\": [7, 9],  # Nombre de voisins\n",
    "        \"weights\": [\"uniform\", \"distance\"],  # Poids des voisins\n",
    "        \"metric\": [\"euclidean\", \"manhattan\"]  # Distance utilis√©e\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        \"n_estimators\": [100, 200],  # Nombre d'arbres\n",
    "        \"max_depth\": [10, 20],  # Profondeur des arbres\n",
    "        \"min_samples_split\": [5, 10],  # Nombre min d'√©chantillons pour diviser un n≈ìud\n",
    "        \"min_samples_leaf\": [2]  # Nombre min d'√©chantillons par feuille\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"n_estimators\": [100, 200],\n",
    "        \"learning_rate\": [0.01, 0.1],\n",
    "        \"max_depth\": [5, 7],\n",
    "        \"subsample\": [1]  # Ratio d'√©chantillonnage des donn√©es\n",
    "    }\n",
    "}\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")\n",
    "}\n",
    "\n",
    "def optimize_and_evaluate(model_name, model, param_grid, X_train, y_train, X_test, y_test):\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring=\"accuracy\", n_jobs=-1, verbose=1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    return {\n",
    "        \"Mod√®le\": model_name,\n",
    "        \"Meilleurs Param√®tres\": str(grid_search.best_params_),\n",
    "        \"Accuracy\": round(acc, 4),\n",
    "        \"Precision\": round(precision, 4),\n",
    "        \"Recall\": round(recall, 4),\n",
    "        \"F1-score\": round(f1, 4)\n",
    "    }\n",
    "\n",
    "print(\"\\nüîç Optimisation et √©valuation des mod√®les - Actions (Stocks) üîç\")\n",
    "results_stocks = []\n",
    "for name, model in models.items():\n",
    "    results_stocks.append(optimize_and_evaluate(name, model, param_grids[name], X_train_stocks, y_train_stocks, X_test_stocks, y_test_stocks))\n",
    "\n",
    "print(\"\\nüîç Optimisation et √©valuation des mod√®les - ETFs üîç\")\n",
    "results_etfs = []\n",
    "for name, model in models.items():\n",
    "    results_etfs.append(optimize_and_evaluate(name, model, param_grids[name], X_train_etfs, y_train_etfs, X_test_etfs, y_test_etfs))\n",
    "\n",
    "df_results_stocks = pd.DataFrame(results_stocks)\n",
    "df_results_etfs = pd.DataFrame(results_etfs)\n",
    "\n",
    "print(\"\\nüìä R√©sultats des mod√®les - Actions (Stocks)\")\n",
    "print(df_results_stocks)\n",
    "\n",
    "print(\"\\nüìä R√©sultats des mod√®les - ETFs\")\n",
    "print(df_results_etfs)\n",
    "\n",
    "def plot_results(df, title):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    df_plot = df.melt(id_vars=[\"Mod√®le\"], value_vars=[\"Accuracy\", \"Precision\", \"Recall\", \"F1-score\"])\n",
    "    sns.barplot(x=\"Mod√®le\", y=\"value\", hue=\"variable\", data=df_plot, palette=\"viridis\")\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Mod√®le\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title=\"M√©trique\")\n",
    "    plt.show()\n",
    "\n",
    "plot_results(df_results_stocks, \"üìä Comparaison des mod√®les - Actions (Stocks)\")\n",
    "\n",
    "plot_results(df_results_etfs, \"üìä Comparaison des mod√®les - ETFs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance + Matrice de confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "feature_models = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")\n",
    "}\n",
    "\n",
    "for name, model in feature_models.items():\n",
    "    model.fit(X_train_stocks, y_train_stocks)  # Entra√Ænement du mod√®le\n",
    "    importance = model.feature_importances_\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.barplot(x=X_train_stocks.columns, y=importance, palette=\"Blues_r\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title(f\"Feature Importance ({name}) - Actions (Stocks)\")\n",
    "    plt.xlabel(\"Features\")\n",
    "    plt.ylabel(\"Importance\")\n",
    "    plt.show()\n",
    "\n",
    "for name, model in feature_models.items():\n",
    "    model.fit(X_train_etfs, y_train_etfs)  # Entra√Ænement du mod√®le\n",
    "    importance = model.feature_importances_\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.barplot(x=X_train_etfs.columns, y=importance, palette=\"Reds_r\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title(f\"Feature Importance ({name}) - ETFs\")\n",
    "    plt.xlabel(\"Features\")\n",
    "    plt.ylabel(\"Importance\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"coolwarm\", xticklabels=[\"0\", \"1\"], yticklabels=[\"0\", \"1\"])\n",
    "    plt.xlabel(\"Pr√©dictions\")\n",
    "    plt.ylabel(\"Vraies valeurs\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "for name, model in models_stocks.items():\n",
    "    model.fit(X_train_stocks, y_train_stocks)\n",
    "    y_pred = model.predict(X_test_stocks)\n",
    "    plot_confusion_matrix(y_test_stocks, y_pred, f\"Matrice de Confusion - {name} (Stocks)\")\n",
    "\n",
    "for name, model in models_etfs.items():\n",
    "    model.fit(X_train_etfs, y_train_etfs)\n",
    "    y_pred = model.predict(X_test_etfs)\n",
    "    plot_confusion_matrix(y_test_etfs, y_pred, f\"Matrice de Confusion - {name} (ETFs)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
